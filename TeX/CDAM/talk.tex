\documentclass[pdf, unicode, ucs, notheorems]{beamer}

\usetheme{Madrid}
\usefonttheme[onlymath]{serif}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{footline}[frame number]
\setbeamerfont{page number in head/foot}{series=\bfseries}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{ragged2e}
\usepackage{wrapfig}
\usepackage{t-angles}
\usepackage{slashbox}
\usepackage{hhline}
\usepackage{multirow}
\usepackage{graphics}
\usepackage{color}
\usepackage{tikz}
\usetikzlibrary{tikzmark, calc, fit}
\include{letters_series_mathbb}

\setbeamercolor{bluetext_color}{fg=blue}
\newcommand{\bluetext}[1]{{\usebeamercolor[fg]{bluetext_color}#1}}
\newcommand{\mathclap}[1]{\text{\smash{\clap{$#1$}}}}

\graphicspath{{./img}}

\newtheorem{theorem}{Theorem}
\newtheorem{statement}{Statement}

\theoremstyle{definition}
\newtheorem{definition}{Definition}

\title[Tensor SSA]{Tensors for signal and frequency estimation in
subspace-based methods: when they are useful?}

\author[Khromov N., Golyandina N.]{\texorpdfstring{\underline{Nikita
Khromov}}{Nikita Khromov}, Nina Golyandina}

\institute[SPbU]{%
  \small
  \vspace{0.2cm}\\
  St.\,Petersburg State University\\
  Department of Statistical Modeling\\
  \vspace{0.1cm}
}

\date{\small DD.09.2025, CDAM'2025}

\begin{document}

\begin{frame}[plain]
  \titlepage
\end{frame}

\begin{frame}{Introduction to Singular Spectrum Analysis (SSA)}
  Problems that can be solved by SSA-related methods:
  \begin{itemize}
      \bluetext{
      \item Signal extraction
      \item Frequency estimation
      }
    \item Smoothing and Noise reduction
    \item Signal decomposition (Trend and Periodicity extraction)
    \item Forecasting
    \item Missing data imputation
    \item Change in structure detection
    \item Many others\dots
  \end{itemize}

\end{frame}

\begin{frame}{SSA Materials}
  Books:
  \begin{itemize}
    \item J.Elsner and A.Tsonis. Singular Spectrum Analysis: A New Tool in
      Time Series Analysis, Plenum, 1996.
    \item N.Golyandina, V.Nekrutrin and A.Zhigljavsky. Analysis of Time
      Series Structure: SSA and Related Techniques, CRC Press, 2001.
    \item S.Sanei and H.Hassani. Singular Spectrum Analysis for Biomedical
      Signals, CRC Press, 2016.
    \item N.Golyandina, A.Korobeynikov and A.Zhigljavsky. Singular spectrum
      analysis with R, Springer, 2018.
    \item N.Golyandina and A.Zhigljavsky. Singular Spectrum Analysis for
      Time Series, Springer, 2013, 2020 (2nd Edition).
  \end{itemize}

  \bigskip

  Implementations:
  \begin{itemize}
    \item R Package: Rssa \\
      \hspace{2ex} https://CRAN.R-project.org/package=Rssa
    \item Python Package: py-ssa-lib (less features) \\
      \hspace{2ex} https://pypi.org/project/py-ssa-lib
  \end{itemize}
\end{frame}

\begin{frame}{SSA Decomposition example}
  Decomposition of time series:
  \begin{itemize}
    \item Low-frequency component + high-frequency component
    \item \bluetext{Signal + noise}
    \item Trend + Seasonality + Noise
  \end{itemize}

  \begin{figure}[!ht]
    \center
    \includegraphics[draft, width=0.8\textwidth,
    height=0.5\textheight]{example-image}\\
    *Some data*: demonstration of series decomposition with SSA
  \end{figure}
\end{frame}

\begin{frame}{ESPRIT Frequency estimation example}
  ESPRIT --- SSA-related method for parameters estimation

  \begin{figure}[!ht]
    \centering
    \includegraphics[draft, width=0.8\textwidth,
    height=0.5\textheight]{example-image}\\
    *Pole motion data probably*
  \end{figure}

  \begin{enumerate}
    \item Estimate --- interpretation
    \item Estimate --- interpretation
  \end{enumerate}
\end{frame}

\begin{frame}{Complex Time Series}
  Common origins of complex-valued time series:
  \begin{itemize}
    \item Can be constructed from two related features
    \item Arise as a result of applying the Fourier transform to real data
  \end{itemize}
\end{frame}

\begin{frame}{SSA Algorithm: Embedding}
  \textbf{Input:} time series $\tX = (x_1,\, x_2,\, \ldots, x_N)$,
  window length $L$, signal rank $r$.
  \vspace{0.4cm}\\
  \begin{enumerate}
    \item \textbf{Embedding}.
      Constructing the $L$-\emph{Trajectory} Hankel matrix $\bfX\in
      \bbC^{L \times K}$ from the series $\tX$, where $K = N - L + 1$:\\
      \[
        \bfX = \calT^{(L)}(\tX) =
        \begin{pmatrix}
          x_1                     & \tikzmarknode{A12}{x_2} &
          \tikzmarknode{A13}{x_3} & \ldots & x_K     \\
          \tikzmarknode{A21}{x_2} & x_3                     & x_4
          & \ldots & x_{K+1} \\
          \tikzmarknode{A31}{x_3} & x_4                     & x_5
          & \ldots & x_{K+2} \\
          \vdots                  & \vdots                  & \vdots
          & \ddots & \vdots  \\
          x_L                     & x_{L+1}                 & x_{L+2}
          & \ldots & x_N
        \end{pmatrix}
      \]
      \begin{tikzpicture}[remember picture,overlay]
        \draw[red] let \p1=($(A12)-(A21)$),\n1={atan2(\y1,\x1)} in
        node[rotate fit=\n1,fit=(A12) (A21),draw,rounded
        corners,inner sep=2pt]{};
        \draw[blue] let \p1=($(A13)-(A31)$),\n1={atan2(\y1,\x1)+2} in
        node[rotate fit=\n1,fit=(A13) (A31),draw,rounded
        corners,inner sep=2pt]{};
      \end{tikzpicture}
  \end{enumerate}
\end{frame}

\begin{frame}{SSA Algorithm: Decomposition, Grouping, Reconstruction}
  \begin{enumerate}
      \setcounter{enumi}{1}
    \item \textbf{Decomposition}.
      Constructing the singular value decomposition (SVD) of matrix $\bfX$:
      $\displaystyle \bfX = \sum_{j=1}^{\operatorname{rank}\bfX}
      \sqrt{\lambda_j} U_j V_j^{\rmH} =
      \sum_{j=1}^{\operatorname{rank}\bfX} \widehat{\bfX}_j$
      where $\rmH$ denotes Hermitian
      conjugation, $U_j$ and $V_j$ are left and right singular vectors
      of $\bfX$, $\sqrt{\lambda_j}$ --- its singular values in
      descending order.
      \vspace{0.2cm}\\
    \item \textbf{Grouping}. Grouping the terms $\widehat{\bfX}_j$
      from the decomposition related to the signal:
      $\displaystyle \bfS = \sum_{j=1}^{r}\widehat{\bfX}_j = \Pi_r \bfX$,
      where $\Pi_r$ is the projector onto the space of matrices with
      rank not greater than $r$.
      \vspace{0.2cm}
    \item \textbf{Reconstruction}. Applying projection onto the
      space of Hankel
      matrices: $\widetilde{\bfS}= \Pi_{\calH}\widehat{\bfS}$,
      and return to the series form: $\widetilde{\tS}=
      \left(\calT^{(L)}\right)^{-1}(\widetilde{\bfS})$
  \end{enumerate}
\end{frame}

\begin{frame}{Series rank}
  \begin{definition}
    Series $\tX$ has rank $d < N/2$, if the rank of its $L$-trajectory
    matrix equals $d$ for any $L$ such that $d \leqslant \min(L,
    N - L + 1)$.\\
    If such $d$ exists, then $\tX$ is called a series of finite rank.
  \end{definition}
  \vspace{0.5cm}
  If the signal $\tS$ is a series of finite rank, then it is
  generally recommended to use $\operatorname{rank}(\tS)$ as
  parameter $r$ in the SSA method
  \vspace{0.4cm}\\
  Series rank examples
  \begin{itemize}
    \item rank of $\tS$ with $s_n = A\sin(2\pi \omega n + \varphi)$,
      $0 < \omega < 1/2$, equals 2
    \item rank of $\tS$ with $s_n = A \exp(\alpha n)$, $\alpha
      \in\bbC$, equals 1
  \end{itemize}
\end{frame}

\begin{frame}{Signal Model}
  What we consider a signal $\tS = (s_1,\, s_2,\, \ldots, s_N)$:\smallskip
  \begin{itemize}
    \item The trajectory matrix $\bfS = \calT^{(L)}(\tS)$ is rank-deficient \\
      ($\implies$ the time series is of some finite rank:
      $\operatorname{rank}(\tS)=r$)
      \medskip
    \item Any signal $\tS$ can be represented in the form of a finite sum:
      \[
        s_n = \sum_{j} p_j(n) \exp(\alpha_j n + \mathrm{i}(2\pi
        \omega_j n + \varphi_j)),
      \]
      where $p_j(n)$ is a polynomial in $n$
      \medskip
    \item Real case:
      \[
        s_n = \sum_{j} p_j(n) \exp(\alpha_j n)\sin(2\pi
        \omega_j n + \varphi_j),
      \]
  \end{itemize}
  \bigskip
  ESPRIT method estimates damping factors $\alpha_j$ and
  frequencies $\omega_j$
\end{frame}

\begin{frame}{ESPRIT Algorithm: General Idea}
  \[
    s_n = \sum_{j=1}^{2} \exp(\alpha_j n + \mathrm{i}(2\pi
    \omega_j n + \varphi_j)) = A_1 z_1^n + A_2 z_2^n
  \]
  where $A_j = \exp(\mathrm{i} \varphi_j)$, $z_j = \exp(\alpha_j +
  \mathrm{i}2\pi\omega_j)$ \vspace{0.4cm}\\
  Signal subspace basis is given by
  \[
    \bfM =
    \begin{pmatrix}
      z_1 & z_2 \\
      z_1^2 & z_2^2\\
      \vdots & \vdots \\
      z_1^L & z_2^L
    \end{pmatrix} \Rightarrow \overline{\bfM} = \underline{\bfM}
    \begin{pmatrix}
      z_1 & \\ & z_2
    \end{pmatrix}\Rightarrow  \underline{\bfM}^{-}\overline{\bfM} =
    \begin{pmatrix}
      z_1 & \\ & z_2
    \end{pmatrix}
  \]
  where $\overline{\bfM}$ denotes $\bfM$ without the first row,
  $\underline{\bfM}$ --- without the last\\
  $\underline{\bfM}^{-}$ denotes the pseudoinverse of $\underline{\bfM}$
\end{frame}

\begin{frame}{ESPRIT Algorithm}
  \textbf{Input}: same as in SSA: $\tX$, $L$, $r$
  \begin{enumerate}
    \item \textbf{Embedding}. $\bfX = \calT^{(L)}(\tX)$
    \item \textbf{Decomposition}.
      $\displaystyle\bfX = \sum_{j=1}^{\operatorname{rank}\bfX}
      \sqrt{\lambda_j} U_j V_j^{\rmH}$,
      $\bfU_r = \left[U_1: U_2: \ldots: U_r\right]$
    \item \textbf{Estimation}. Finding eigenvalues $z_j$ of matrix
      $\underline{\bfU}_r^{-}\overline{\bfU}_r$ \\ \smallskip
      From $z_j = \exp(\alpha_j + \mathrm{i}2\pi\omega_j)$
      parameters $\alpha_j$ and $\omega_j$ can be found
  \end{enumerate}
\end{frame}

\begin{frame}{Multi-Channel Time Series, MSSA}
  $\tX = \left(\tX^{(1)},\, \tX^{(2)}, \, \ldots,\,
  \tX^{(P)}\right), \qquad\tX^{(p)} = \left(x_1^{(p)}, \, x_2^{(p)},\,
  \ldots,\,  x_N^{(p)}\right)$ -- channels
  \vspace{0.4cm}

  The only change in the algorithms --- Embedding step:
  \[
    \bfX = \calT_{\textrm{MSSA}}^{(L)}(\tX) = \left[\bfX^{(1)} :
    \bfX^{(2)}: \ldots : \bfX^{(P)}\right],
  \]
  \[
    \bfX^{(p)} = \calT^{(L)}(\tX^{(p)})
  \]
  \vspace{0.4cm}

  When to chose MSSA over SSA for each channel:
  \begin{itemize}
    \item All channels have ''similar'' structure
    \item ''Supporting'' channels with lower noise level
  \end{itemize}
\end{frame}

\begin{frame}{Intoduction of Tensors}
  \begin{table}
    \hspace*{-0.9cm}
    \begin{tabular}{rlllll}
      \bluetext{Basic SSA:} & Time series $\tX$ & $\mapsto$ & Matrix
      $\mathbf{X}$ & $\mapsto$ & $\operatorname{SVD}(\mathbf{X})$ \\
      \bluetext{Tensor SSA:} & Time series $\tX$ & $\mapsto$ & Tensor
      $\calX $ & $\mapsto$ & \hspace*{-0.18cm}
      $\underbrace{\operatorname{TD}}_{\mathclap{\text{~~~Some Tensor
      Decomposition}}}(\calX)$
    \end{tabular}
  \end{table}
  \vspace{0.4cm}

  Tensor SVD Extensions:
  \begin{itemize}
    \item \bluetext{Higher-Order SVD (HOSVD)}
    \item Canonical Polyadic Decomposition (CPD)
    \item T-SVD
    \item $(L_r, L_r, 1)$-Decomposition
  \end{itemize}
\end{frame}

\begin{frame}{Mapping Time Series to Tensor}
  \begin{tikzpicture}[remember picture, overlay]
    \node[right=3.8cm, below=0.6cm] at (current page.north west)
    {
      \includegraphics[width=0.7\textwidth]{tens-injection-wide.pdf}
    };
  \end{tikzpicture}
  \begin{columns}
    \begin{column}{0.65\textwidth}

    \end{column}
    \begin{column}{0.3\textwidth}
      $\tX = (x_1,\, x_2,\, \ldots,\, x_N)$\\\medskip
      $\tX \mapsto \calT_{\mathrm{T-SSA}}^{(I, L)}(\tX) = \calX$\\\medskip
      $\calX \in \bbC^{I\times L \times K}$\\\smallskip $K = N - I - L + 2$
    \end{column}
    \begin{column}{0.05\textwidth}

    \end{column}
  \end{columns}
  \vspace{2.5cm}

  \begin{tikzpicture}[remember picture, overlay]
    \node[left=4.5cm, above=0.3cm] at (current page.south east)
    {
      \includegraphics[width=0.7\textwidth]{mssa_injection_new.pdf}
    };
  \end{tikzpicture}
  \begin{columns}
    \begin{column}{0.01\textwidth}

    \end{column}
    \begin{column}{0.4\textwidth}
      $\tX = \left(\tX^{(1)},\, \tX^{(2)},\, \ldots,\,
      \tX^{(P)}\right)$\\\medskip
      $\tX \mapsto \calT_{\mathrm{T-MSSA}}^{(L)}(\tX) = \calX$\\\medskip
      $\calX \in \bbC^{P\times L \times K}$\\\smallskip $K = N - L + 1$
    \end{column}
    \begin{column}{0.59\textwidth}

    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Higher-Order SVD. Higher-Order Orthogonal Iterations}
  \vspace*{-0.7cm}
  \begin{align*}
    \operatorname{SVD}(\bfX) &= \sum_{j=1}^{\operatorname{rank}(\bfX)}
    \sqrt{\lambda_j} U_j V_j^{\rmH}\\
    \operatorname{HOSVD}(\calX) &=
    \sum_{i=1}^{\operatorname{rank}_{\color{red}1}(\calX)}\,
    \sum_{l=1}^{\operatorname{rank}_{\color{red}2}(\calX)}\,
    \sum_{k=1}^{\operatorname{rank}_{\color{red}3}(\calX)}\,
    \calZ_{ilk} U_i^{(1)} \circ U_l^{(2)} \circ U_k^{(3)}
  \end{align*}

  \begin{itemize}
    \item $\displaystyle \widetilde{\bfX} =
      \sum_{j=1}^{R}\ldots \Rightarrow
      \left\| \bfX - \widetilde{\bfX} \right\|_{F} =
      \min_{\operatorname{rank}(\widehat{\bfX}) \leqslant R}\left\|
      \bfX - \widehat{\bfX}\right\|_F$
    \item $\displaystyle \widetilde{\calX}=
      \sum_{i=1}^{R_1}\,
      \sum_{l=1}^{R_2}\,
      \sum_{k=1}^{R_3}\,
      \ldots \Rightarrow
      \left\| \calX - \widetilde{\calX} \right\|_{F} \geqslant
      \min_{\operatorname{rank}_{m}(\widehat{\calX}) \leqslant R_m}\left\|
      \calX - \widehat{\calX}\right\|_F$
  \end{itemize}
  \vspace{0.6cm}

  \bluetext{Truncation of SVD is optimal, but truncation of HOSVD is
  not}\vspace{0.1cm}

  Iterative algorithm for finding optimal approximation -- \bluetext{HOOI}
\end{frame}

\begin{frame}{Higher-Order SSA, MSSA, ESPRIT}
  \textbf{Input:} time series $\tX$,
  window length: $(I, L)$ for single-channel or $L$ for multi-channel,
  signal ranks $(r_1,\, r_2,\, r_3)$, $d$ --- estimation dimension
  for HO-ESPRIT.
  \vspace{0.4cm}\\
  \begin{enumerate}
    \item \textbf{Embedding}.
      \begin{tabular}{r|l}
        Single-channel & $\tX \mapsto\calT_{\mathrm{T-SSA}}^{(I,
        L)}(\tX) = \calX$ \\ \hline
        Multi-channel &
        $\tX \mapsto \calT_{\mathrm{T-MSSA}}^{(L)}(\tX) = \calX$
      \end{tabular}
      \vspace{0.2cm}

    \item \textbf{Decomposition \& Approximation}. Using $(r_1,\,
      r_2,\, r_3)$\\\smallskip
      $\calX \mapsto
      \operatorname{Trunc}(\operatorname{HOSVD}(\calX)) = \widetilde{\calS}$ or
      $\calX \mapsto \operatorname{HOOI}(\calX) = \widetilde{\calS}$
      \vspace{0.2cm}

    \item \textbf{Reconstruction or Estimation}. \\
      \begin{itemize}
        \item \textbf{Reconstruction}.
          $\tS = \left(\calT\right)^{-1}
          \left(\Pi_{\calH_{\mathrm{T}}} (\widetilde{\calS})\right)$,
          $\Pi_{\calH_{\mathrm{T}}}$ --- projector onto the space of
          Hankel tensors
        \item \textbf{Estimation}. Finding eigenvalues $z_j$ of
          matrix $\underline{\bfU}^{-}\overline{\bfU}$, where
          $\bfU = \left[U_1^{(d)}:U_2^{(d)}:\ldots :U_{r_d}^{(d)}\right]$.
          From $z_j = \exp(\alpha_j + \mathrm{i}2\pi\omega_j)$
          damping factors $\alpha_j$ and frequencies $\omega_j$ of
          the signal can be found
      \end{itemize}

  \end{enumerate}
\end{frame}
\end{document}